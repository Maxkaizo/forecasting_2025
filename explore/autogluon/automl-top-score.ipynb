{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T15:07:25.043364Z",
     "iopub.status.busy": "2025-02-16T15:07:25.042896Z",
     "iopub.status.idle": "2025-02-16T15:07:25.049266Z",
     "shell.execute_reply": "2025-02-16T15:07:25.04818Z",
     "shell.execute_reply.started": "2025-02-16T15:07:25.043326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import duckdb as db\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T15:10:53.63496Z",
     "iopub.status.busy": "2025-02-16T15:10:53.634571Z",
     "iopub.status.idle": "2025-02-16T15:10:59.408689Z",
     "shell.execute_reply": "2025-02-16T15:10:59.407239Z",
     "shell.execute_reply.started": "2025-02-16T15:10:53.634929Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3781/1861528378.py:3: UserWarning: Parsing dates in %d.%m.%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  test = pd.read_csv('data/test.csv', parse_dates=['date'], sep=';')\n"
     ]
    }
   ],
   "source": [
    "sales = pd.read_csv('data/sales.csv', parse_dates=['date'])\n",
    "online = pd.read_csv('data/online.csv', parse_dates=['date'])\n",
    "test = pd.read_csv('data/test.csv', parse_dates=['date'], sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = db.connect(\"data/mydb.duckdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T15:07:45.748515Z",
     "iopub.status.busy": "2025-02-16T15:07:45.748009Z",
     "iopub.status.idle": "2025-02-16T15:07:45.760719Z",
     "shell.execute_reply": "2025-02-16T15:07:45.758582Z",
     "shell.execute_reply.started": "2025-02-16T15:07:45.748478Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfefecf0e0a84f5f84c3fd54df5defd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_sales = db.sql(f\"\"\"\n",
    "    with date_dim as (\n",
    "        select distinct date \n",
    "        from (\n",
    "            select date from sales \n",
    "            union \n",
    "            select date from online\n",
    "        )\n",
    "    ),\n",
    "    store_item as (\n",
    "        select distinct \n",
    "            store_id,\n",
    "            item_id\n",
    "        from (\n",
    "            select store_id, item_id from sales\n",
    "            union\n",
    "            select store_id, item_id from online\n",
    "        )\n",
    "    ),\n",
    "    date_store_item_cross as (\n",
    "        select \n",
    "            d.date,\n",
    "            si.store_id,\n",
    "            si.item_id\n",
    "        from date_dim d\n",
    "        cross join store_item si\n",
    "    ),\n",
    "    daily_sales as (\n",
    "        select \n",
    "            dsi.date,\n",
    "            dsi.item_id,\n",
    "            extract(month from dsi.date) as month,\n",
    "            extract(year from dsi.date) as year,\n",
    "            extract(day from dsi.date) as day,\n",
    "            extract(week from dsi.date) as week,\n",
    "            case \n",
    "                when extract(dow from dsi.date) in (0, 6) then 'weekend'\n",
    "                else 'weekday'\n",
    "            end as day_type,\n",
    "            1 as sample_weight,\n",
    "            extract(dow from dsi.date) as day_of_week,\n",
    "            dsi.store_id,\n",
    "            coalesce(a.quantity, 0) + coalesce(b.quantity, 0) as quantity\n",
    "        from date_store_item_cross dsi\n",
    "        left join sales as a \n",
    "            on dsi.date = a.date\n",
    "            and dsi.item_id = a.item_id\n",
    "            and dsi.store_id = a.store_id\n",
    "        left join online as b \n",
    "            on dsi.date = b.date\n",
    "            and dsi.item_id = b.item_id\n",
    "            and dsi.store_id = b.store_id\n",
    "    )\n",
    "    \n",
    "    select \n",
    "        *,\n",
    "        coalesce(avg(quantity) over (\n",
    "            partition by store_id, item_id\n",
    "            order by date \n",
    "            rows between 7 preceding and 1 preceding\n",
    "        ), 0) as rolling_7day_avg,\n",
    "        coalesce(avg(quantity) over (\n",
    "            partition by store_id, item_id\n",
    "            order by date \n",
    "            rows between 14 preceding and 1 preceding\n",
    "        ), 0) as rolling_14day_avg,\n",
    "        coalesce(avg(quantity) over (\n",
    "            partition by store_id, item_id\n",
    "            order by date \n",
    "            rows between 30 preceding and 1 preceding\n",
    "        ), 0) as rolling_30day_avg\n",
    "    from daily_sales\n",
    "    order by date\n",
    "\"\"\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1227/1470816164.py:7: UserWarning: Parsing dates in %d.%m.%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  test = pd.read_csv('data/test.csv', parse_dates=['date'], sep=';')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9757651138432fbfe284a02bf9bef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date       item_id  month  year  day  week day_type  sample_weight  \\\n",
      "0 2022-08-28  00c76368f791      8  2022   28    34  weekend              1   \n",
      "1 2022-08-28  020b151fb4fd      8  2022   28    34  weekend              1   \n",
      "2 2022-08-28  05db904338e0      8  2022   28    34  weekend              1   \n",
      "3 2022-08-28  0af42b55f108      8  2022   28    34  weekend              1   \n",
      "4 2022-08-28  0e71cef8e293      8  2022   28    34  weekend              1   \n",
      "\n",
      "   day_of_week  store_id  quantity  rolling_7day_avg  rolling_14day_avg  \\\n",
      "0            0         1       0.0               0.0                0.0   \n",
      "1            0         1       0.0               0.0                0.0   \n",
      "2            0         1       0.0               0.0                0.0   \n",
      "3            0         1       0.0               0.0                0.0   \n",
      "4            0         1       0.0               0.0                0.0   \n",
      "\n",
      "   rolling_30day_avg  \n",
      "0                0.0  \n",
      "1                0.0  \n",
      "2                0.0  \n",
      "3                0.0  \n",
      "4                0.0  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d7a55bf866473aa43b18c60631cc17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7de0281ca5f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar datos originales en DataFrames\n",
    "sales = pd.read_csv('data/sales.csv', parse_dates=['date'])\n",
    "online = pd.read_csv('data/online.csv', parse_dates=['date'])\n",
    "test = pd.read_csv('data/test.csv', parse_dates=['date'], sep=';')\n",
    "\n",
    "# Abrir conexión a un archivo DuckDB (persistente en disco)\n",
    "con = duckdb.connect(\"data/mydb.duckdb\")\n",
    "\n",
    "# Registrar DataFrames como tablas temporales en DuckDB\n",
    "con.register(\"sales\", sales)\n",
    "con.register(\"online\", online)\n",
    "\n",
    "# Crear tabla persistente con tu query\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE combined_sales AS\n",
    "    with date_dim as (\n",
    "        select distinct date \n",
    "        from (\n",
    "            select date from sales \n",
    "            union \n",
    "            select date from online\n",
    "        )\n",
    "    ),\n",
    "    store_item as (\n",
    "        select distinct \n",
    "            store_id,\n",
    "            item_id\n",
    "        from (\n",
    "            select store_id, item_id from sales\n",
    "            union\n",
    "            select store_id, item_id from online\n",
    "        )\n",
    "    ),\n",
    "    date_store_item_cross as (\n",
    "        select \n",
    "            d.date,\n",
    "            si.store_id,\n",
    "            si.item_id\n",
    "        from date_dim d\n",
    "        cross join store_item si\n",
    "    ),\n",
    "    daily_sales as (\n",
    "        select \n",
    "            dsi.date,\n",
    "            dsi.item_id,\n",
    "            extract(month from dsi.date) as month,\n",
    "            extract(year from dsi.date) as year,\n",
    "            extract(day from dsi.date) as day,\n",
    "            extract(week from dsi.date) as week,\n",
    "            case \n",
    "                when extract(dow from dsi.date) in (0, 6) then 'weekend'\n",
    "                else 'weekday'\n",
    "            end as day_type,\n",
    "            1 as sample_weight,\n",
    "            extract(dow from dsi.date) as day_of_week,\n",
    "            dsi.store_id,\n",
    "            coalesce(a.quantity, 0) + coalesce(b.quantity, 0) as quantity\n",
    "        from date_store_item_cross dsi\n",
    "        left join sales as a \n",
    "            on dsi.date = a.date\n",
    "            and dsi.item_id = a.item_id\n",
    "            and dsi.store_id = a.store_id\n",
    "        left join online as b \n",
    "            on dsi.date = b.date\n",
    "            and dsi.item_id = b.item_id\n",
    "            and dsi.store_id = b.store_id\n",
    "    )\n",
    "    \n",
    "    select \n",
    "        *,\n",
    "        coalesce(avg(quantity) over (\n",
    "            partition by store_id, item_id\n",
    "            order by date \n",
    "            rows between 7 preceding and 1 preceding\n",
    "        ), 0) as rolling_7day_avg,\n",
    "        coalesce(avg(quantity) over (\n",
    "            partition by store_id, item_id\n",
    "            order by date \n",
    "            rows between 14 preceding and 1 preceding\n",
    "        ), 0) as rolling_14day_avg,\n",
    "        coalesce(avg(quantity) over (\n",
    "            partition by store_id, item_id\n",
    "            order by date \n",
    "            rows between 30 preceding and 1 preceding\n",
    "        ), 0) as rolling_30day_avg\n",
    "    from daily_sales\n",
    "    order by date\n",
    "\"\"\")\n",
    "\n",
    "# Ahora la tabla queda guardada en \"data/mydb.duckdb\"\n",
    "# Puedes consultar después sin volver a calcular todo\n",
    "df = con.execute(\"SELECT * FROM combined_sales LIMIT 10\").df()\n",
    "print(df.head())\n",
    "\n",
    "# Exportar también a Parquet si quieres\n",
    "con.execute(\"COPY combined_sales TO 'data/combined_sales.parquet' (FORMAT 'parquet')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T15:09:21.570626Z",
     "iopub.status.busy": "2025-02-16T15:09:21.570262Z",
     "iopub.status.idle": "2025-02-16T15:09:23.990336Z",
     "shell.execute_reply": "2025-02-16T15:09:23.98897Z",
     "shell.execute_reply.started": "2025-02-16T15:09:21.570596Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "# combined_sales[(combined_sales['store_id'] == 1) & (combined_sales['item_id'] == 'b0d24502fb66')].head(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>2022-08-28 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <td>00c76368f791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>week</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day_type</th>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sample_weight</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day_of_week</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store_id</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantity</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rolling_7day_avg</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rolling_14day_avg</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rolling_30day_avg</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     0\n",
       "date               2022-08-28 00:00:00\n",
       "item_id                   00c76368f791\n",
       "month                                8\n",
       "year                              2022\n",
       "day                                 28\n",
       "week                                34\n",
       "day_type                       weekend\n",
       "sample_weight                        1\n",
       "day_of_week                          0\n",
       "store_id                             1\n",
       "quantity                           0.0\n",
       "rolling_7day_avg                   0.0\n",
       "rolling_14day_avg                  0.0\n",
       "rolling_30day_avg                  0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['store_id'] == 1) & (df['item_id'] == '00c76368f791')].head(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T15:11:03.744574Z",
     "iopub.status.busy": "2025-02-16T15:11:03.74414Z",
     "iopub.status.idle": "2025-02-16T15:11:06.205682Z",
     "shell.execute_reply": "2025-02-16T15:11:06.203917Z",
     "shell.execute_reply.started": "2025-02-16T15:11:03.744538Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test = db.sql(\"\"\"\n",
    "    select \n",
    "        a.row_id,\n",
    "        a.item_id,\n",
    "        a.store_id,\n",
    "        extract(month from a.date) as month,\n",
    "        extract(year from a.date) as year,\n",
    "        extract(day from a.date) as day,\n",
    "        extract(week from a.date) as week,\n",
    "        extract(dayofweek from a.date) as day_of_week,\n",
    "        coalesce(avg(b.rolling_7day_avg), 0) as rolling_7day_avg,\n",
    "        coalesce(avg(b.rolling_14day_avg), 0) as rolling_14day_avg,\n",
    "        coalesce(avg(b.rolling_30day_avg), 0) as rolling_30day_avg\n",
    "    from test as a \n",
    "    left join combined_sales as b  \n",
    "        on a.item_id = b.item_id\n",
    "        and a.store_id = b.store_id\n",
    "        and b.week in (38,39)\n",
    "        and b.year = 2024\n",
    "    group by 1,2,3,4,5,6,7,8 \n",
    "\"\"\").df()\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1227/3530420566.py:7: UserWarning: Parsing dates in %d.%m.%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  test_df = pd.read_csv('data/test.csv', parse_dates=['date'], sep=';')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32735510de484088bca125c58b4b2c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id       item_id  store_id  month  year  day  week  day_of_week  \\\n",
      "0  372929  9454fb3e603d         2     10  2024   26    43            6   \n",
      "1  490919  ccc1d739eb10         3     10  2024   26    43            6   \n",
      "2  486599  fbc35fe0e09d         3     10  2024   26    43            6   \n",
      "3  110159  3460f346e8d6         1     10  2024   26    43            6   \n",
      "4  669449  fee521eec807         4     10  2024   26    43            6   \n",
      "\n",
      "   rolling_7day_avg  rolling_14day_avg  rolling_30day_avg  \n",
      "0          0.246753           0.155844           0.169697  \n",
      "1          1.831169           1.948052           1.830303  \n",
      "2          0.389610           0.402597           0.578788  \n",
      "3          3.242429           3.383305           3.106106  \n",
      "4          2.103896           2.064935           1.557576  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7de0213029b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar datos originales\n",
    "sales = pd.read_csv('data/sales.csv', parse_dates=['date'])\n",
    "online = pd.read_csv('data/online.csv', parse_dates=['date'])\n",
    "test_df = pd.read_csv('data/test.csv', parse_dates=['date'], sep=';')\n",
    "\n",
    "# Conectar a la base DuckDB en disco\n",
    "con = duckdb.connect(\"data/mydb.duckdb\")\n",
    "\n",
    "# Registrar DataFrames como tablas temporales\n",
    "con.register(\"sales\", sales)\n",
    "con.register(\"online\", online)\n",
    "con.register(\"test\", test_df)\n",
    "\n",
    "# (IMPORTANTE: aquí asumimos que la tabla combined_sales ya fue creada en disco con el script anterior)\n",
    "\n",
    "# Crear tabla persistente para test\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE test_features AS\n",
    "    select \n",
    "        a.row_id,\n",
    "        a.item_id,\n",
    "        a.store_id,\n",
    "        extract(month from a.date) as month,\n",
    "        extract(year from a.date) as year,\n",
    "        extract(day from a.date) as day,\n",
    "        extract(week from a.date) as week,\n",
    "        extract(dayofweek from a.date) as day_of_week,\n",
    "        coalesce(avg(b.rolling_7day_avg), 0) as rolling_7day_avg,\n",
    "        coalesce(avg(b.rolling_14day_avg), 0) as rolling_14day_avg,\n",
    "        coalesce(avg(b.rolling_30day_avg), 0) as rolling_30day_avg\n",
    "    from test as a \n",
    "    left join combined_sales as b  \n",
    "        on a.item_id = b.item_id\n",
    "        and a.store_id = b.store_id\n",
    "        and b.week in (38,39)\n",
    "        and b.year = 2024\n",
    "    group by 1,2,3,4,5,6,7,8 \n",
    "\"\"\")\n",
    "\n",
    "# Consultar la tabla persistente\n",
    "df_test = con.execute(\"SELECT * FROM test_features LIMIT 10\").df()\n",
    "print(df_test.head())\n",
    "\n",
    "# Exportar a parquet si lo deseas\n",
    "con.execute(\"COPY test_features TO 'data/test_features.parquet' (FORMAT 'parquet')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T15:11:09.887915Z",
     "iopub.status.busy": "2025-02-16T15:11:09.887516Z",
     "iopub.status.idle": "2025-02-16T15:11:11.161391Z",
     "shell.execute_reply": "2025-02-16T15:11:11.160236Z",
     "shell.execute_reply.started": "2025-02-16T15:11:09.887884Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['item_id', \n",
    "            'store_id', \n",
    "            'day', \n",
    "            'week', \n",
    "            'day_of_week',\n",
    "            'rolling_7day_avg',\t\n",
    "            'rolling_14day_avg',\t\n",
    "            'rolling_30day_avg', \n",
    "            'quantity',]\n",
    "\n",
    "train = df[features]\n",
    "train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T15:11:14.029336Z",
     "iopub.status.busy": "2025-02-16T15:11:14.028917Z",
     "iopub.status.idle": "2025-02-16T15:11:31.105143Z",
     "shell.execute_reply": "2025-02-16T15:11:31.104285Z",
     "shell.execute_reply.started": "2025-02-16T15:11:14.0293Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = db.sql(\"\"\"\n",
    "    WITH quantiles AS (\n",
    "        SELECT \n",
    "            item_id,\n",
    "            store_id,\n",
    "            day_of_week,\n",
    "            percentile_cont(0.01) WITHIN GROUP (ORDER BY quantity) as q01,\n",
    "            percentile_cont(0.99) WITHIN GROUP (ORDER BY quantity) as q99\n",
    "        FROM train\n",
    "        GROUP BY item_id, store_id, day_of_week\n",
    "    )\n",
    "    SELECT t.*\n",
    "    FROM train t\n",
    "    JOIN quantiles q \n",
    "        ON t.item_id = q.item_id \n",
    "        AND t.store_id = q.store_id\n",
    "        AND t.day_of_week = q.day_of_week\n",
    "    WHERE t.quantity > q.q01 \n",
    "        AND t.quantity < q.q99\n",
    "\"\"\").df()\n",
    "\n",
    "train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78b80e4368341c2856774a41632f153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7de0213029b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE train AS\n",
    "    SELECT \n",
    "        item_id,\n",
    "        store_id,\n",
    "        day,\n",
    "        week,\n",
    "        day_of_week,\n",
    "        rolling_7day_avg,\t\n",
    "        rolling_14day_avg,\t\n",
    "        rolling_30day_avg, \n",
    "        quantity\n",
    "    FROM combined_sales\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb57bbb7c524aa0b6e8db36ba2539df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7de0213029b0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE train_filtered AS\n",
    "    WITH quantiles AS (\n",
    "        SELECT \n",
    "            item_id,\n",
    "            store_id,\n",
    "            day_of_week,\n",
    "            percentile_cont(0.01) WITHIN GROUP (ORDER BY quantity) as q01,\n",
    "            percentile_cont(0.99) WITHIN GROUP (ORDER BY quantity) as q99\n",
    "        FROM train\n",
    "        GROUP BY item_id, store_id, day_of_week\n",
    "    )\n",
    "    SELECT t.*\n",
    "    FROM train t\n",
    "    JOIN quantiles q \n",
    "        ON t.item_id = q.item_id \n",
    "        AND t.store_id = q.store_id\n",
    "        AND t.day_of_week = q.day_of_week\n",
    "    WHERE t.quantity > q.q01 \n",
    "      AND t.quantity < q.q99\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_sales: 44363256 filas\n",
      "test_features: 883680 filas\n",
      "train: 44363256 filas\n",
      "train_filtered: 6562913 filas\n"
     ]
    }
   ],
   "source": [
    "print(\"combined_sales:\", con.execute(\"SELECT COUNT(*) FROM combined_sales\").fetchone()[0], \"filas\")\n",
    "print(\"test_features:\", con.execute(\"SELECT COUNT(*) FROM test_features\").fetchone()[0], \"filas\")\n",
    "print(\"train:\", con.execute(\"SELECT COUNT(*) FROM train\").fetchone()[0], \"filas\")\n",
    "print(\"train_filtered:\", con.execute(\"SELECT COUNT(*) FROM train_filtered\").fetchone()[0], \"filas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T15:10:28.393737Z",
     "iopub.status.busy": "2025-02-16T15:10:28.393209Z",
     "iopub.status.idle": "2025-02-16T15:10:28.418127Z",
     "shell.execute_reply": "2025-02-16T15:10:28.417149Z",
     "shell.execute_reply.started": "2025-02-16T15:10:28.393697Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250826_053402\"\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(label='quantity', eval_metric='root_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:01:09.266502Z",
     "iopub.status.busy": "2025-02-16T14:01:09.265609Z",
     "iopub.status.idle": "2025-02-16T14:01:09.271953Z",
     "shell.execute_reply": "2025-02-16T14:01:09.270494Z",
     "shell.execute_reply.started": "2025-02-16T14:01:09.266434Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tree_hyperparameters = {\n",
    "    'GBM': {},   \n",
    "    'XGB': {},   \n",
    "    'RF': {},    \n",
    "    'XT': {},    \n",
    "    'CAT': {}    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b492965c2d34adc9ad1adf1a0b2ece3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = con.execute(\"SELECT * FROM train_filtered\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:01:21.266337Z",
     "iopub.status.busy": "2025-02-16T14:01:21.265871Z",
     "iopub.status.idle": "2025-02-16T14:04:29.259419Z",
     "shell.execute_reply": "2025-02-16T14:04:29.258166Z",
     "shell.execute_reply.started": "2025-02-16T14:01:21.266302Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.12.8\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025\n",
      "CPU Count:          4\n",
      "Memory Avail:       1.35 GB / 5.79 GB (23.3%)\n",
      "Disk Space Avail:   882.30 GB / 1006.85 GB (87.6%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 900s\n",
      "AutoGluon will save models to \"/home/maxkaizo/forecasting_2025/explore/autogluon/AutogluonModels/ag-20250826_053402\"\n",
      "Train Data Rows:    6562913\n",
      "Train Data Columns: 8\n",
      "Label Column:       quantity\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (4736.0, -150.0, 5.77516, 27.03577)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1607.32 MB\n",
      "\tTrain Data (Original)  Memory Usage: 732.29 MB (45.6% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 45.6% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 3 | ['rolling_7day_avg', 'rolling_14day_avg', 'rolling_30day_avg']\n",
      "\t\t('int', [])    : 4 | ['store_id', 'day', 'week', 'day_of_week']\n",
      "\t\t('object', []) : 1 | ['item_id']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 1 | ['item_id']\n",
      "\t\t('float', [])    : 3 | ['rolling_7day_avg', 'rolling_14day_avg', 'rolling_30day_avg']\n",
      "\t\t('int', [])      : 4 | ['store_id', 'day', 'week', 'day_of_week']\n",
      "\t56.3s = Fit runtime\n",
      "\t8 features in original data used to generate 8 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 363.02 MB (12.4% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 60.88s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.01, Train Rows: 6497283, Val Rows: 65630\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'RF': [{}],\n",
      "\t'XT': [{}],\n",
      "\t'CAT': [{}],\n",
      "}\n",
      "Fitting 5 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM ... Training model for up to 838.81s of the 836.51s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 2.101 GB out of 1.693 GB available memory (124.078%)... (90.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.43 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train LightGBM... Skipping this model.\n",
      "Fitting model: RandomForest ... Training model for up to 827.53s of the 825.35s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 4.034 GB out of 1.563 GB available memory (258.068%)... (50.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=5.21 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train RandomForest... Skipping this model.\n",
      "Fitting model: CatBoost ... Training model for up to 826.21s of the 824.03s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 2.476 GB out of 1.561 GB available memory (158.623%)... (100.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.64 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train CatBoost... Skipping this model.\n",
      "Fitting model: ExtraTrees ... Training model for up to 825.43s of the 823.25s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 4.034 GB out of 1.560 GB available memory (258.511%)... (50.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=5.22 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train ExtraTrees... Skipping this model.\n",
      "Fitting model: XGBoost ... Training model for up to 824.64s of the 822.46s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 2.901 GB out of 1.558 GB available memory (186.165%)... (100.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.91 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train XGBoost... Skipping this model.\n",
      "No base models to train on, skipping auxiliary stack level 2...\n",
      "Warning: AutoGluon did not successfully train any models\n",
      "AutoGluon training complete, total runtime = 91.83s ... Best model: None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No models were trained successfully during fit(). Inspect the log output or increase verbosity to determine why no models were fit. Alternatively, set `raise_on_no_models_fitted` to False during the fit call.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictor\u001b[38;5;241m.\u001b[39mfit(train, \n\u001b[1;32m      2\u001b[0m               presets\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedium_quality\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      3\u001b[0m               time_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m15\u001b[39m, \n\u001b[1;32m      4\u001b[0m               verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \n\u001b[1;32m      5\u001b[0m               hyperparameters \u001b[38;5;241m=\u001b[39m tree_hyperparameters)\n",
      "File \u001b[0;32m~/miniconda3/envs/forecasting/lib/python3.12/site-packages/autogluon/core/utils/decorators.py:31\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     30\u001b[0m     gargs, gkwargs \u001b[38;5;241m=\u001b[39m g(\u001b[38;5;241m*\u001b[39mother_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39mgargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/forecasting/lib/python3.12/site-packages/autogluon/tabular/predictor/predictor.py:1363\u001b[0m, in \u001b[0;36mTabularPredictor.fit\u001b[0;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, fit_full_last_level_weighted_ensemble, full_weighted_ensemble_additionally, dynamic_stacking, calibrate_decision_threshold, num_cpus, num_gpus, fit_strategy, memory_limit, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;66;03m# keep track of the fit strategy used for future calls\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_strategy \u001b[38;5;241m=\u001b[39m fit_strategy\n\u001b[0;32m-> 1363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(ag_fit_kwargs\u001b[38;5;241m=\u001b[39mag_fit_kwargs, ag_post_fit_kwargs\u001b[38;5;241m=\u001b[39mag_post_fit_kwargs)\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/forecasting/lib/python3.12/site-packages/autogluon/tabular/predictor/predictor.py:1371\u001b[0m, in \u001b[0;36mTabularPredictor._fit\u001b[0;34m(self, ag_fit_kwargs, ag_post_fit_kwargs)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_learner\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag_fit_kwargs)\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_post_fit_vars()\n\u001b[0;32m-> 1371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_fit(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag_post_fit_kwargs)\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/miniconda3/envs/forecasting/lib/python3.12/site-packages/autogluon/tabular/predictor/predictor.py:1703\u001b[0m, in \u001b[0;36mTabularPredictor._post_fit\u001b[0;34m(self, keep_only_best, refit_full, set_best_to_refit_full, save_space, calibrate, calibrate_decision_threshold, infer_limit, num_cpus, num_gpus, refit_full_kwargs, fit_strategy, raise_on_no_models_fitted)\u001b[0m\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_names():\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_no_models_fitted:\n\u001b[0;32m-> 1703\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1704\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo models were trained successfully during fit().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1705\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Inspect the log output or increase verbosity to determine why no models were fit.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1706\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Alternatively, set `raise_on_no_models_fitted` to False during the fit call.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1707\u001b[0m         )\n\u001b[1;32m   1709\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m30\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: No models found, skipping post_fit logic...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No models were trained successfully during fit(). Inspect the log output or increase verbosity to determine why no models were fit. Alternatively, set `raise_on_no_models_fitted` to False during the fit call."
     ]
    }
   ],
   "source": [
    "predictor.fit(train, \n",
    "              presets='medium_quality', \n",
    "              time_limit=60*15, \n",
    "              verbosity=2, \n",
    "              hyperparameters = tree_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:04:43.570852Z",
     "iopub.status.busy": "2025-02-16T14:04:43.570213Z",
     "iopub.status.idle": "2025-02-16T14:05:08.085493Z",
     "shell.execute_reply": "2025-02-16T14:05:08.084429Z",
     "shell.execute_reply.started": "2025-02-16T14:04:43.570729Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(predictor.feature_importance(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:05:19.272284Z",
     "iopub.status.busy": "2025-02-16T14:05:19.271857Z",
     "iopub.status.idle": "2025-02-16T14:05:19.329736Z",
     "shell.execute_reply": "2025-02-16T14:05:19.32845Z",
     "shell.execute_reply.started": "2025-02-16T14:05:19.272246Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "features_test = ['item_id', \n",
    "            'store_id', \n",
    "            'day', \n",
    "            'week', \n",
    "            'day_of_week',\n",
    "            'rolling_7day_avg',\t\n",
    "            'rolling_14day_avg',\t\n",
    "            'rolling_30day_avg', \n",
    "            ]\n",
    "\n",
    "test[features_test].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T13:31:14.32984Z",
     "iopub.status.busy": "2025-02-16T13:31:14.329449Z",
     "iopub.status.idle": "2025-02-16T13:34:25.773804Z",
     "shell.execute_reply": "2025-02-16T13:34:25.772619Z",
     "shell.execute_reply.started": "2025-02-16T13:31:14.329815Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "test['quantity'] = predictor.predict(test[features_test])\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T13:34:42.819207Z",
     "iopub.status.busy": "2025-02-16T13:34:42.818785Z",
     "iopub.status.idle": "2025-02-16T13:34:43.037163Z",
     "shell.execute_reply": "2025-02-16T13:34:43.035203Z",
     "shell.execute_reply.started": "2025-02-16T13:34:42.819175Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('/kaggle/input/ml-zoomcamp-2024-competition/sample_submission.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T13:34:48.893195Z",
     "iopub.status.busy": "2025-02-16T13:34:48.89283Z",
     "iopub.status.idle": "2025-02-16T13:34:48.992569Z",
     "shell.execute_reply": "2025-02-16T13:34:48.990926Z",
     "shell.execute_reply.started": "2025-02-16T13:34:48.893166Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# join on item_id, store_id, date\n",
    "submission = db.sql(\"\"\"\n",
    "    select \n",
    "        a.row_id,\n",
    "        b.quantity\n",
    "    from submission as a\n",
    "    left join test as b\n",
    "        on a.row_id = b.row_id\n",
    "\"\"\").df()\n",
    "submission.head(), submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T13:34:51.032355Z",
     "iopub.status.busy": "2025-02-16T13:34:51.031995Z",
     "iopub.status.idle": "2025-02-16T13:34:51.83395Z",
     "shell.execute_reply": "2025-02-16T13:34:51.83192Z",
     "shell.execute_reply.started": "2025-02-16T13:34:51.032328Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission.to_csv('submission_autogluon_best_recent_min_with_rolling_averages.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T18:05:42.594777Z",
     "iopub.status.busy": "2025-02-09T18:05:42.59442Z",
     "iopub.status.idle": "2025-02-09T18:05:42.64145Z",
     "shell.execute_reply": "2025-02-09T18:05:42.640202Z",
     "shell.execute_reply.started": "2025-02-09T18:05:42.594748Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission.info()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10175212,
     "sourceId": 88022,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
